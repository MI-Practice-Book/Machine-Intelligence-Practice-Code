import argparse
import json
import math
import os
from dataclasses import dataclass
from typing import List, Dict, Any

import torch
from torch.utils.data import Dataset, Subset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model

# --- 1. æ•°æ®é›†å®šä¹‰ ---
class MessagesDataset(Dataset):
    """è¯»å–æ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼çš„æ•°æ®é›† [cite: 226, 230]"""
    def __init__(self, path: str):
        self.items = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                self.items.append(json.loads(line))

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        return self.items[idx]["messages"]

# --- 2. æ ¸å¿ƒï¼šAssistant-Only Loss æ•°æ®æ•´ç†å™¨ ---
@dataclass
class DataCollatorAssistantOnly:
    """
    å®ç°æŸå¤±æ©ç æœºåˆ¶ï¼šä»…å¯¹ assistant å›å¤éƒ¨åˆ†è®¡ç®—äº¤å‰ç†µæŸå¤± [cite: 213, 215, 247]
    """
    tokenizer: Any
    max_length: int = 1024

    def __call__(self, batch_messages: List[List[Dict[str, str]]]):
        input_ids_list = []
        labels_list = []
        attn_list = []

        for messages in batch_messages:
            # ä½¿ç”¨èŠå¤©æ¨¡æ¿æ¸²æŸ“å®Œæ•´å¯¹è¯ [cite: 204, 206]
            full_text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            # æ¸²æŸ“ä¸å«ç­”æ¡ˆçš„å‰ç¼€éƒ¨åˆ†ï¼Œç”¨äºå®šä½ç­”æ¡ˆèµ·å§‹ä½ç½® [cite: 252, 253]
            msgs_prefix = messages[:-1] 
            prefix_text = self.tokenizer.apply_chat_template(
                msgs_prefix, tokenize=False, add_generation_prompt=True
            )

            full = self.tokenizer(full_text, truncation=True, max_length=self.max_length, padding=False)
            prefix = self.tokenizer(prefix_text, truncation=True, max_length=self.max_length, padding=False)

            input_ids = full["input_ids"]
            attention_mask = full["attention_mask"]
            prefix_len = len(prefix["input_ids"])

            # æ„é€ æ ‡ç­¾ï¼šå°†å‰ç¼€éƒ¨åˆ†ï¼ˆSystem/Userï¼‰è®¾ä¸º -100 ä»¥å¿½ç•¥æŸå¤±è®¡ç®— [cite: 247, 255]
            labels = list(input_ids)
            for i in range(min(prefix_len, len(labels))):
                labels[i] = -100
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ ‡ç­¾ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªé -100 çš„æ ‡ç­¾ï¼‰
            if all(l == -100 for l in labels):
                # å¦‚æœæ‰€æœ‰æ ‡ç­¾éƒ½è¢«æ©ç ï¼Œè‡³å°‘ä¿ç•™æœ€åä¸€ä¸ª token ç”¨äºè®¡ç®—æŸå¤±
                if len(labels) > 0:
                    labels[-1] = input_ids[-1]

            input_ids_list.append(input_ids)
            labels_list.append(labels)
            attn_list.append(attention_mask)

        # Padding å¤„ç† [cite: 254]
        pad_id = self.tokenizer.pad_token_id
        max_len = max(len(x) for x in input_ids_list)
        
        def pad(seq, pad_value):
            return seq + [pad_value] * (max_len - len(seq))

        return {
            "input_ids": torch.tensor([pad(x, pad_id) for x in input_ids_list], dtype=torch.long),
            "attention_mask": torch.tensor([pad(x, 0) for x in attn_list], dtype=torch.long),
            "labels": torch.tensor([pad(x, -100) for x in labels_list], dtype=torch.long),
        }

# --- 3. è®­ç»ƒä¸»æµç¨‹ ---
def main():
    p = argparse.ArgumentParser()
    p.add_argument("--model", default="Qwen/Qwen2.5-0.5B-Instruct")
    p.add_argument("--data", default="data/train_messages.jsonl")
    p.add_argument("--out", default="outputs/sft_lora")
    p.add_argument("--epochs", type=int, default=1)
    args = p.parse_args()

    # åŠ è½½åˆ†è¯å™¨ [cite: 304]
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æ¨¡å‹å¹¶åº”ç”¨æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ [cite: 197, 269]
    print(f">>> æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        device_map="auto",
        dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    )
    
    # ä¼˜åŒ–2ï¼šå¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜å³°å€¼ [cite: 269]
    model.gradient_checkpointing_enable()

    # é…ç½® LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ [cite: 271, 308, 311]
    lora = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora)
    model.print_trainable_parameters()

    # å‡†å¤‡æ•°æ®é›†å¹¶åˆ‡åˆ†éªŒè¯é›†ç”¨äºè¯„æµ‹ 
    full_dataset = MessagesDataset(args.data)
    num_val = min(int(len(full_dataset) * 0.05), 50)  # æœ€å¤šå–50æ¡ä½œä¸ºéªŒè¯é›†ï¼ŒèŠ‚çœæ˜¾å­˜
    train_ds = Subset(full_dataset, range(num_val, len(full_dataset)))
    eval_ds = Subset(full_dataset, range(num_val)) if num_val > 0 else None
    
    collator = DataCollatorAssistantOnly(tokenizer=tokenizer, max_length=512)  # å‡å°æœ€å¤§é•¿åº¦èŠ‚çœæ˜¾å­˜

    # è®­ç»ƒè¶…å‚æ•°é…ç½® [cite: 265, 268]
    train_args = TrainingArguments(
        output_dir=args.out,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=1,  # è¯„ä¼°æ—¶ä½¿ç”¨æ›´å°çš„ batch size
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        num_train_epochs=args.epochs,
        logging_steps=10,
        save_steps=100,
        bf16=torch.cuda.is_available(),
        report_to="none",
        dataloader_pin_memory=False,  # ç¦ç”¨ pin_memory èŠ‚çœæ˜¾å­˜
        max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        warmup_steps=50,  # å­¦ä¹ ç‡é¢„çƒ­ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
    )

    # å®ä¾‹åŒ– Trainer [cite: 234, 256]
    trainer = Trainer(
        model=model,
        args=train_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    # æ‰§è¡Œè®­ç»ƒ [cite: 218]
    print(">>> å¼€å§‹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰...")
    trainer.train()

    # æ€§èƒ½è¯„ä¼°ï¼šè®¡ç®—éªŒè¯é›†å›°æƒ‘åº¦ (PPL) [cite: 273]
    if eval_ds is not None:
        print(">>> æ­£åœ¨æ‰§è¡Œæœ€ç»ˆè¯„ä¼°...")
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        try:
            metrics = trainer.evaluate()
            eval_loss = metrics.get("eval_loss", float("nan"))
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºæœ‰æ•ˆå€¼
            if math.isnan(eval_loss) or math.isinf(eval_loss):
                print("âš ï¸  è¯„ä¼°æŸå¤±ä¸º NaN/Infï¼Œå¯èƒ½æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°äº†æ•°å€¼ä¸ç¨³å®š")
                print("   å»ºè®®ï¼šæ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼Œé™ä½å­¦ä¹ ç‡ï¼Œæˆ–æ£€æŸ¥æ•°æ®æ ¼å¼")
            else:
                try:
                    ppl = math.exp(eval_loss)
                    print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼éªŒè¯é›† Loss: {eval_loss:.4f}, å›°æƒ‘åº¦ (PPL): {ppl:.2f}")
                except OverflowError:
                    print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼éªŒè¯é›† Loss: {eval_loss:.4f}, å›°æƒ‘åº¦ (PPL): æº¢å‡º")
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âš ï¸  è¯„ä¼°æ—¶æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡æœ€ç»ˆè¯„ä¼°")
            else:
                raise
    else:
        print("ğŸ“Š è®­ç»ƒå®Œæˆï¼")

    # ä¿å­˜æƒé‡ä¸åˆ†è¯å™¨ [cite: 318, 321]
    trainer.save_model(args.out)
    tokenizer.save_pretrained(args.out)
    print(f"âœ… æ¨¡å‹ä¸é€‚é…å™¨å·²ä¿å­˜è‡³: {args.out}")

if __name__ == "__main__":
    main()