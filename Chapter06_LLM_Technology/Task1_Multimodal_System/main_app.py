import os
from typing import Tuple, List

import torch
from PIL import Image
from torch.utils.data import DataLoader

from dataset_loader import Flickr8kDataset
from model_core import ClipCaptionModel, load_clip_model

# --- ç›¸å¯¹è·¯å¾„é…ç½® ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_DIR = "./data/Flicker8k_Dataset"
TOKEN_FILE = "./data/Flickr8k_text/Flickr8k.token.txt"
# è¿™é‡ŒæŒ‡å‘åˆšæ‰è®­ç»ƒä¿å­˜çš„æƒé‡ (å¦‚æœè®­ç»ƒäº†5è½®ï¼Œå°±é€‰epoch_5)
MODEL_PATH = "./checkpoints/caption_model_epoch_5.pt" 


def build_image_index(
    clip_model, loader: DataLoader, device: str, max_batches: int = 3
) -> Tuple[torch.Tensor, List[str]]:
    """
    é¢„æå–ä¸€å°éƒ¨åˆ†å›¾åƒç‰¹å¾ï¼Œä½œä¸ºæ£€ç´¢ç”¨çš„â€œå‘é‡æ•°æ®åº“â€ã€‚
    ä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œé»˜è®¤åªå–å‰ max_batches ä¸ª batchã€‚
    ç”±äº Flickr8k æ¯å¼ å›¾ç‰‡æœ‰å¤šæ¡æ ‡æ³¨ï¼Œè¿™é‡ŒæŒ‰å›¾ç‰‡è·¯å¾„å»é‡ï¼Œä»¥é¿å…åŒå›¾é‡å¤å‡ºç°åœ¨ TopKã€‚
    """
    img_feats, img_paths = [], []
    seen = set()
    with torch.no_grad():
        for i, batch in enumerate(loader):
            pixel_values = batch["pixel_values"].to(device)
            feats = clip_model.get_image_features(pixel_values)
            feats = feats / feats.norm(p=2, dim=-1, keepdim=True)

            # é€æ¡æŒ‰å›¾ç‰‡è·¯å¾„å»é‡
            for feat, path in zip(feats, batch["img_path"]):
                if path in seen:
                    continue
                seen.add(path)
                img_feats.append(feat.unsqueeze(0))
                img_paths.append(path)

            if i + 1 >= max_batches:
                break

    if not img_feats:
        raise RuntimeError("æœªèƒ½æ„å»ºå›¾åƒå‘é‡ç´¢å¼•ï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½ã€‚")

    return torch.cat(img_feats, dim=0), img_paths

def main():
    print(f"\n>>> [ç³»ç»Ÿå¯åŠ¨] æ­£åœ¨åŠ è½½æ•°æ®ä¸æ¨¡å‹ (Device: {DEVICE})...")

    # 1. åŸºç¡€æ£€æŸ¥
    if not os.path.exists(IMG_DIR):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {IMG_DIR}ï¼Œè¯·æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•ã€‚")
        return

    dataset = Flickr8kDataset(IMG_DIR, TOKEN_FILE)
    loader = DataLoader(dataset, batch_size=32, shuffle=False)

    # 2. åŠ è½½æ¨¡å‹
    clip_model = load_clip_model(DEVICE)  # æ£€ç´¢ç”¨
    caption_model = ClipCaptionModel().to(DEVICE)  # ç”Ÿæˆç”¨

    # [å…³é”®] åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    if os.path.exists(MODEL_PATH):
        print(f">>> æ­£åœ¨åŠ è½½è®­ç»ƒæƒé‡: {MODEL_PATH}")
        state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
        caption_model.load_state_dict(state_dict)
        print(">>> æƒé‡åŠ è½½æˆåŠŸï¼")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ {MODEL_PATH}")
        print("âš ï¸ ç”Ÿæˆçš„æè¿°å°†æ˜¯ä¹±ç ï¼è¯·å…ˆè¿è¡Œ train_model.pyã€‚")

    caption_model.eval()

    # 3. é¢„æ„å»ºå°å‹å›¾åƒç´¢å¼•ï¼Œç”¨äºè·¨æ¨¡æ€æ£€ç´¢
    print("\n>>> [ç´¢å¼•æ„å»º] æ­£åœ¨å¯¹å›¾åƒæå–ç‰¹å¾...")
    img_features, img_paths = build_image_index(clip_model, loader, DEVICE, max_batches=3)
    print(f">>> å·²æ„å»º {len(img_paths)} å¼ å›¾ç‰‡çš„å‘é‡ç´¢å¼•ï¼Œå¯ç”¨äºæ–‡æœ¬æ£€ç´¢ã€‚")

    # 4. å¾ªç¯äº¤äº’å¼é€‰æ‹©ä»»åŠ¡
    while True:
        print("\n" + "=" * 40)
        print("è¯·é€‰æ‹©ä»»åŠ¡ï¼š")
        print("1) æ–‡æœ¬ -> æ‰¾å›¾ (è·¨æ¨¡æ€æ£€ç´¢)")
        print("2) å›¾ç‰‡ -> ç”Ÿæˆæè¿° (çœ‹å›¾è¯´è¯)")
        print("0) é€€å‡º")
        print("=" * 40)
        task = input("è¯·è¾“å…¥ 0 / 1 / 2ï¼š").strip()

        if task == "0":
            print("å·²é€€å‡ºã€‚")
            break

        if task == "1":
            # --- åŠŸèƒ½Aï¼šä»¥æ–‡æœå›¾ ---
            query = input("è¯·è¾“å…¥æ£€ç´¢æ–‡æœ¬ï¼š").strip()
            if not query:
                print("âŒ æœªè¾“å…¥æ–‡æœ¬ï¼Œè¯·é‡è¯•ã€‚")
                continue

            text_inputs = dataset.processor(text=[query], return_tensors="pt", padding=True).to(DEVICE)
            with torch.no_grad():
                text_features = clip_model.get_text_features(**text_inputs)
                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)

            similarity = (text_features @ img_features.T).softmax(dim=-1)
            k = min(3, img_features.shape[0])
            values, indices = similarity[0].topk(k)

            print("\nâœ… æ£€ç´¢ç»“æœï¼ˆTop 3ï¼‰ï¼š")
            for rank, (score, idx) in enumerate(zip(values, indices), start=1):
                print(f"{rank}. ç›¸ä¼¼åº¦: {score.item():.4f} | å›¾ç‰‡: {img_paths[idx]}")

        elif task == "2":
            # --- åŠŸèƒ½Bï¼šçœ‹å›¾è¯´è¯ ---
            img_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼š").strip()
            if not os.path.exists(img_path):
                print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶: {img_path}")
                continue

            try:
                image = Image.open(img_path).convert("RGB")
            except Exception as exc:
                print(f"âŒ æ— æ³•æ‰“å¼€å›¾ç‰‡: {exc}")
                continue

            # è·å–ç›®æ ‡å›¾ç‰‡ç‰¹å¾
            with torch.no_grad():
                img_inputs = dataset.processor(images=image, return_tensors="pt").to(DEVICE)
                target_feat = clip_model.get_image_features(**img_inputs)
                target_feat = target_feat / target_feat.norm(p=2, dim=-1, keepdim=True)

                prefix = caption_model(target_feat.float())
                generated_ids = caption_model.gpt.generate(
                    inputs_embeds=prefix,
                    max_length=30,
                    num_beams=5,
                    no_repeat_ngram_size=2,
                    early_stopping=True,
                )

            output_text = dataset.processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            print("\nğŸ¤– AIæè¿°:", output_text)
        else:
            print("âŒ æœªçŸ¥ä»»åŠ¡é€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

    print("=" * 40)

if __name__ == "__main__":
    main()